{"cells": [{"cell_type": "code", "execution_count": 8, "id": "c212dc7e-806a-4905-89bf-4cacb2b5d2a2", "metadata": {}, "outputs": [], "source": "from google.cloud import storage"}, {"cell_type": "code", "execution_count": 59, "id": "87824d20-ea6b-4e0f-8685-41bc0c912912", "metadata": {}, "outputs": [], "source": "import os"}, {"cell_type": "code", "execution_count": 6, "id": "a11f7c36-c66b-4d6c-8677-60a0996536c4", "metadata": {"tags": []}, "outputs": [], "source": "#Importer les biblioth\u00e8ques n\u00e9cessaires (pandas et pyspark) dans le script pyspark\nimport pandas as pd"}, {"cell_type": "code", "execution_count": 7, "id": "73e8e13c-cae2-4c95-9ab1-4677efe47db8", "metadata": {"tags": []}, "outputs": [{"ename": "ModuleNotFoundError", "evalue": "No module named 'openpyxl'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_3078/1020030620.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenpyxl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"]}], "source": "import openpyxl"}, {"cell_type": "code", "execution_count": null, "id": "a4461f77-6b60-4f43-a8a6-84f4fc9d7b6d", "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import lit"}, {"cell_type": "code", "execution_count": null, "id": "3df454ae-dc8d-4b85-a7a9-667034f2c835", "metadata": {"tags": []}, "outputs": [], "source": "import pyspark.sql\nfrom pyspark.sql import *"}, {"cell_type": "code", "execution_count": null, "id": "eff94251-05b8-4964-ab56-5eb95f98114d", "metadata": {"tags": []}, "outputs": [], "source": "import requests"}, {"cell_type": "code", "execution_count": null, "id": "4601cc9f-7fb3-4e12-b650-2cb5dea85139", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark import SparkFiles"}, {"cell_type": "code", "execution_count": null, "id": "94c4127a-6976-427d-a920-fd8eaae21e22", "metadata": {"tags": []}, "outputs": [], "source": "import zipfile\nimport io\nimport os"}, {"cell_type": "code", "execution_count": null, "id": "77d36493-2ea2-4e5e-87de-b928a1f70219", "metadata": {"tags": []}, "outputs": [], "source": "#Cr\u00e9ation de la session spark \nspark = SparkSession.builder.appName(\"Excel to DataFrame\").getOrCreate() "}, {"cell_type": "code", "execution_count": null, "id": "cdd065bf-d67c-49dd-a3ea-abdfc240e6b3", "metadata": {"tags": []}, "outputs": [], "source": "def TelechargementData():\n\n    url = \"https://services.eaufrance.fr/documents/openData/SISPEA_FR_\"+str(annee)+\"_AEP.zip\"\n\n    reponse = requests.get(url)\n\n    content = reponse.content\n\n    # Cr\u00e9ation d'un objet ZipFile \u00e0 partir du contenu ZIP t\u00e9l\u00e9charg\u00e9\n    zip_file = zipfile.ZipFile(io.BytesIO(content))\n\n    # Cr\u00e9ation du r\u00e9pertoire de destination s'il n'existe pas\n    os.makedirs(\"Data\", exist_ok=True)\n\n    # Extraction des fichiers du fichier ZIP et enregistrement dans le r\u00e9pertoire de destination\n    zip_file.extractall(\"Data\")\n    \n    # Fermeture du fichier ZIP\n    zip_file.close()"}, {"cell_type": "code", "execution_count": null, "id": "b9da2b25-4b47-4126-b75d-ff002a0b839b", "metadata": {"tags": []}, "outputs": [], "source": "def lecturePandas(annee):\n    ''' Cette fonction permet de lire un fichier excel et de le convertir en dataFrame Pandas'''\n\n    #Pour lire le fichier excel on utilise Pandas \n    #On donne le chemin du fichier excel\n    excel_file_path = \"./Data/SISPEA_FR_\"+str(annee)+\"_AEP.xls\"\n    #On donne le nom de l'onglet\n    sheet_name = \"Entit\u00e9s de gestion\"\n    #Lecture du fichier excel avec pandas \n    pandas_df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n\n    return pandas_df"}, {"cell_type": "markdown", "id": "cea0d18e-2389-445c-9871-4da473fecec4", "metadata": {}, "source": "Je supprime la colonne fin de contrat, car elle creer un conflit qui ne me permet pas de convertir mon dataframe en table spark."}, {"cell_type": "code", "execution_count": null, "id": "f9b23b6b-eca4-461b-be49-1389a088e77d", "metadata": {}, "outputs": [], "source": "def supprimerLaColonneDateFin(pandas_df):\n    '''Cette fonction supprime la colonne dateFinDeContrat dans mon dataframe Pandas'''\n    pandas_df.drop([\"Date de fin de contrat\"], axis=1, inplace=True)\n\n    return pandas_df"}, {"cell_type": "code", "execution_count": null, "id": "6047f835-12b1-4e09-b02d-99a70c3f777c", "metadata": {"tags": []}, "outputs": [], "source": "def convertirPandasEnSpark(pandas_df):\n    '''Cette fonction converti la table pandas en table spark'''\n    #Conversion du DataFrame Pandas en DataFrame Spark\n    table = spark.createDataFrame(pandas_df)\n    \n    return table"}, {"cell_type": "code", "execution_count": null, "id": "5cc5c6d8-2f8b-4ef2-b569-42a0134f1818", "metadata": {}, "outputs": [], "source": "def choixColonnes(table_spark):\n    ''' Cette fonction selectionne les colonnes N\u00b0 SIREN,VP.224,VP.225,VP.226,VP.227,VP.228,VP.229,VP.231,VP.232,VP.234 '''\n    \n    table_bonne_colonne=table_spark.select(\"N\u00b0 SIREN\",\"`VP.224`\",\"`VP.225`\", \"`VP.226`\", \"`VP.227`\",\"`VP.228`\",\"`VP.229`\",\"`VP.231`\", \"`VP.232`\",\"`VP.234`\")\n\n    return table_bonne_colonne"}, {"cell_type": "markdown", "id": "2964776f-095f-4f56-83eb-554373d1b4c7", "metadata": {}, "source": "Je renomme les colonnes de mon tableau"}, {"cell_type": "code", "execution_count": null, "id": "afa7ffbd-e50f-4750-93aa-22a537524d5a", "metadata": {}, "outputs": [], "source": "def renommageColonnes(table_spark):\n    ''' Cette fonction renomme les colonnes de la table '''\n\n    table_renamed = table_spark.withColumnRenamed(\"VP.224\", \"Indice lin\u00e9aire de consommation\")\\\n        .withColumnRenamed(\"VP.225\", \"Rendement sur les 3 ann\u00e9es pr\u00e9c\u00e9dentes\")\\\n        .withColumnRenamed(\"VP.226\", \"Rendement seuil par d\u00e9faut\")\\\n        .withColumnRenamed(\"VP.227\", \"Rendement seuil en ZRE\")\\\n        .withColumnRenamed(\"VP.228\", \"Densit\u00e9 lin\u00e9aire d'abonn\u00e9s\")\\\n        .withColumnRenamed(\"VP.229\", \"Ratio habitants par abonn\u00e9s\")\\\n        .withColumnRenamed(\"VP.231\", \"Consommation moyenne par abonn\u00e9\")\\\n        .withColumnRenamed(\"VP.232\", \"Volumes consomm\u00e9s comptabilis\u00e9s\")\\\n        .withColumnRenamed(\"VP.234\", \"Volume produit + Volume import\u00e9\")\n    return table_renamed"}, {"cell_type": "code", "execution_count": null, "id": "f80a4f6b-9103-497b-b7af-4382d1e532d0", "metadata": {}, "outputs": [], "source": "def ajoutcol_annee(table_spark,annee):\n    # Add new constanst column\n    dataframe_colannee = table_spark.withColumn(\"Annee\", lit(annee))\n    return dataframe_colannee"}, {"cell_type": "markdown", "id": "e2435050-78cd-4009-9e7c-1bf9f35df7a7", "metadata": {"tags": []}, "source": "Je sauvegarde mon tableau dans un fichier csv"}, {"cell_type": "code", "execution_count": null, "id": "7f4ebd60-ed9a-47a1-a7b1-a55719a7e0da", "metadata": {"tags": []}, "outputs": [], "source": "def transformationCSV(table_spark):\n    ''' Cette fonction enregistre une table spark au format CSV '''\n    table_spark.write.csv(\"./Local Disk/Data/consommation_eau.csv\", header=True, mode=\"append\")"}, {"cell_type": "code", "execution_count": null, "id": "07678014-bf2c-4974-aeb7-44e5b8c60a48", "metadata": {"tags": []}, "outputs": [], "source": "def main(annee):\n    \n    TelechargementData()\n    \n    pandas_dataframe = lecturePandas(annee)\n\n    pandas_dataframe_propre = supprimerLaColonneDateFin(pandas_dataframe)\n\n    spark_dataframe = convertirPandasEnSpark(pandas_dataframe_propre)\n\n    df_bonnes_colonnes = choixColonnes(spark_dataframe)\n\n    df_bons_noms = renommageColonnes(df_bonnes_colonnes)\n    \n    dataframe_colannee = ajoutcol_annee(df_bons_noms, annee)\n\n    transformationCSV(dataframe_colannee)\n    "}, {"cell_type": "code", "execution_count": null, "id": "262db5ff-2d8a-4606-9e3a-a7feb30343cd", "metadata": {"tags": []}, "outputs": [], "source": "# Selection des ann\u00e9es \u00e0 traiter.\nannee_min = 2008\nanne_max = 2021"}, {"cell_type": "code", "execution_count": null, "id": "ab7052ee-5f4f-4b00-b9c5-ebd7b5f3e9e7", "metadata": {"tags": []}, "outputs": [], "source": "#Execution du programme\n\nfor annee in range(annee_min,anne_max+1):\n    main(annee)"}, {"cell_type": "code", "execution_count": 51, "id": "e632cb24-c81c-4194-a0ff-8bc6cd689c8a", "metadata": {}, "outputs": [], "source": "## lien URI du bucket cr\u00e9e \nDATALAKE_PATH = \"gs://code_de_source_lake\"\n"}, {"cell_type": "code", "execution_count": 52, "id": "b26419d0-5c98-4886-a8c3-c5da5e910d43", "metadata": {}, "outputs": [], "source": "## variables pour envoyer mon fichier csv sur mon bucket\n\nBUCKET_NAME = \"code_de_source_lake\"\nlocal_folder_path = \"./Data/Output_data\"\ngcs_file_path = \"consommation_eau.csv\"\n\n"}, {"cell_type": "code", "execution_count": 53, "id": "20a1a67f-9dcb-4f44-b59f-85295c767722", "metadata": {}, "outputs": [], "source": "# Nom du dossier dans le bucket GCS\ngcs_folder_name = 'Output_data_consommationeau'"}, {"cell_type": "code", "execution_count": 54, "id": "7d3885b8-7388-44bf-a73f-db654911d847", "metadata": {}, "outputs": [], "source": "# Initialiser le client de stockage GCP\nclient = storage.Client()"}, {"cell_type": "code", "execution_count": 55, "id": "89051f62-5bf7-48fd-a56e-a695268f2a38", "metadata": {}, "outputs": [], "source": "# R\u00e9cup\u00e9rer le bucket\nbucket = client.bucket(BUCKET_NAME)\n"}, {"cell_type": "code", "execution_count": 56, "id": "0b0d5c31-b670-4498-8b1c-fccf40055738", "metadata": {}, "outputs": [], "source": "# Envoyer le fichier dans le bucket\nblob = bucket.blob(gcs_file_path)"}, {"cell_type": "code", "execution_count": 60, "id": "de2123f0-68f8-4a33-82b1-795be452af0d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Fichier ./Data/Output_data/part-00006-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00006-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv.\nFichier ./Data/Output_data/part-00004-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00004-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv.\nFichier ./Data/Output_data/part-00009-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00009-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv.\nFichier ./Data/Output_data/part-00003-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00003-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv.\nFichier ./Data/Output_data/part-00001-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00001-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv.\nFichier ./Data/Output_data/part-00008-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00008-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv.\nFichier ./Data/Output_data/part-00006-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00006-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv.\nFichier ./Data/Output_data/part-00002-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00002-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv.\nFichier ./Data/Output_data/part-00008-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00008-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv.\nFichier ./Data/Output_data/part-00000-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00000-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv.\nFichier ./Data/Output_data/part-00002-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00002-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv.\nFichier ./Data/Output_data/part-00007-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00007-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv.\nFichier ./Data/Output_data/part-00011-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00011-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv.\nFichier ./Data/Output_data/part-00007-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00007-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv.\nFichier ./Data/Output_data/part-00005-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00005-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv.\nFichier ./Data/Output_data/part-00001-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00001-99a5bfbe-0173-421b-a4e6-e6674b63283a-c000.csv.\nFichier ./Data/Output_data/part-00010-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00010-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv.\nFichier ./Data/Output_data/part-00011-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00011-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv.\nFichier ./Data/Output_data/part-00000-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv envoy\u00e9 vers Output_data_consommationeau/part-00000-c79c1aaa-31e0-4991-8398-ad7cf9f652b9-c000.csv.\n"}], "source": "# Parcourir les fichiers du dossier local\nfor root, dirs, files in os.walk(local_folder_path):\n    for file in files:\n        local_file_path = os.path.join(root, file)\n        gcs_file_path = os.path.join(gcs_folder_name, os.path.relpath(local_file_path, local_folder_path))\n        \n        # Cr\u00e9er un objet Blob dans le bucket\n        blob = bucket.blob(gcs_file_path)\n        \n        # Charger le contenu du fichier local dans le Blob\n        blob.upload_from_filename(local_file_path)\n\n        print(f'Fichier {local_file_path} envoy\u00e9 vers {gcs_file_path}.')"}, {"cell_type": "code", "execution_count": 58, "id": "bdb1a657-a0a2-4fcc-a27d-193ea500b17e", "metadata": {}, "outputs": [], "source": "# J'ouvre mon fichier pour le lire en binaire\n#with open (local_file_path, \"rb\") as fichier:\n#J'upload le contenu de mon fichier dans mon blob\n    #blob.upload_from_file(fichier)\n\n#print(f\"Le fichier {local_file_path} a \u00e9t\u00e9 upload dans le bucket {BUCKET_NAME}.\")"}, {"cell_type": "code", "execution_count": null, "id": "a57d3e4c-ba7e-40d2-abe9-4ad305a11c4b", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}
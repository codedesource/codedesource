{"cells":[{"cell_type":"markdown","id":"b3abf2ca-b9e4-4ff4-97ac-09587d51f445","metadata":{},"source":["## Auteur : Mauchaussee Pauline \n","# Date de création : 2023/08/17  \n","# Présentation :** Ce notebook permet de telecharger des données climatique depuis une liste de stations stockée dans un table BigQuerry et d'envoyer les données en format CSV dans un bucket GCP.\n","\n","## Prérequis :\n","# - Sauvegarder le fichier excel sous forme de CSV\n","# - Un bucket gcp pour le stockage de données. (BUCKET_NAME)\n","# - Une table BigQuerry contenant la liste des station (TABLE_ID)\n","# - Un token API infoclimat\n","\n","\n","## Params:\n","# BUCKET_NAME : nom du bucket GCP, DOIT être dans le même projet.\n","# TABLE_IDE : ID de la table BigQuerry, DOIT être dans le même projet.\n","# KEY_PATH : Chemin vers le fichier JSON permettant de ce connecter au service account"]},{"cell_type":"markdown","id":"8f14199f-6523-41a0-ada3-422bbf446f9c","metadata":{},"source":["## Import des librairies"]},{"cell_type":"code","execution_count":28,"id":"c212dc7e-806a-4905-89bf-4cacb2b5d2a2","metadata":{},"outputs":[],"source":["from google.cloud import storage"]},{"cell_type":"code","execution_count":29,"id":"87824d20-ea6b-4e0f-8685-41bc0c912912","metadata":{},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":30,"id":"a11f7c36-c66b-4d6c-8677-60a0996536c4","metadata":{"tags":[]},"outputs":[],"source":["#Importer les bibliothèques nécessaires (pandas et pyspark) dans le script pyspark\n","import pandas as pd"]},{"cell_type":"code","execution_count":31,"id":"ad84177c-c73b-43c1-aab6-ad2a9619cd55","metadata":{},"outputs":[],"source":["# !pip install openpyxl"]},{"cell_type":"code","execution_count":32,"id":"d5359971-79c0-4367-a509-a0a9643f0a28","metadata":{},"outputs":[],"source":["# !pip install xlrd"]},{"cell_type":"code","execution_count":33,"id":"73e8e13c-cae2-4c95-9ab1-4677efe47db8","metadata":{"tags":[]},"outputs":[],"source":["import openpyxl"]},{"cell_type":"code","execution_count":34,"id":"a4461f77-6b60-4f43-a8a6-84f4fc9d7b6d","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import lit, col, StringType"]},{"cell_type":"code","execution_count":35,"id":"3df454ae-dc8d-4b85-a7a9-667034f2c835","metadata":{"tags":[]},"outputs":[],"source":["import pyspark.sql\n","from pyspark.sql import *"]},{"cell_type":"code","execution_count":36,"id":"eff94251-05b8-4964-ab56-5eb95f98114d","metadata":{"tags":[]},"outputs":[],"source":["import requests"]},{"cell_type":"code","execution_count":37,"id":"4601cc9f-7fb3-4e12-b650-2cb5dea85139","metadata":{"tags":[]},"outputs":[],"source":["from pyspark import SparkFiles"]},{"cell_type":"code","execution_count":38,"id":"94c4127a-6976-427d-a920-fd8eaae21e22","metadata":{"tags":[]},"outputs":[],"source":["import zipfile\n","import io\n","import os"]},{"cell_type":"markdown","id":"ec35367d-bc66-45d3-b96a-bfc4875e17ab","metadata":{},"source":["# Création d'une fonction pour récupérer de la donnée à partir d'une URL "]},{"cell_type":"code","execution_count":39,"id":"cdd065bf-d67c-49dd-a3ea-abdfc240e6b3","metadata":{"tags":[]},"outputs":[],"source":["def TelechargementData():\n","\n","    url = \"https://services.eaufrance.fr/documents/openData/SISPEA_FR_\"+str(annee)+\"_AEP.zip\"\n","\n","    reponse = requests.get(url)\n","\n","    content = reponse.content\n","\n","    # Création d'un objet ZipFile à partir du contenu ZIP téléchargé\n","    zip_file = zipfile.ZipFile(io.BytesIO(content))\n","\n","    # Création du répertoire de destination s'il n'existe pas\n","    os.makedirs(\"Data\", exist_ok=True)\n","\n","    # Extraction des fichiers du fichier ZIP et enregistrement dans le répertoire de destination\n","    zip_file.extractall(\"Data\")\n","    \n","    # Fermeture du fichier ZIP\n","    zip_file.close()"]},{"cell_type":"markdown","id":"b6601be6-91fb-4faf-8907-399e6b5dc146","metadata":{},"source":["# Création d'une fonction pour lire mon fichier excel et conversion en un DataFrame Pandas"]},{"cell_type":"code","execution_count":40,"id":"b9da2b25-4b47-4126-b75d-ff002a0b839b","metadata":{"tags":[]},"outputs":[],"source":["def lecturePandas(annee):\n","    ''' Cette fonction permet de lire un fichier excel et de le convertir en dataFrame Pandas'''\n","\n","    #Pour lire le fichier excel on utilise Pandas \n","    #On donne le chemin du fichier excel\n","    excel_file_path = \"./Data/SISPEA_FR_\"+str(annee)+\"_AEP.xls\"\n","    #On donne le nom de l'onglet\n","    sheet_name = \"Entités de gestion\"\n","    #Lecture du fichier excel avec pandas \n","    pandas_df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n","\n","    return pandas_df"]},{"cell_type":"markdown","id":"cea0d18e-2389-445c-9871-4da473fecec4","metadata":{},"source":["# Je sélectionne les colonnes que l'on souhaite garder"]},{"cell_type":"code","execution_count":41,"id":"f9b23b6b-eca4-461b-be49-1389a088e77d","metadata":{},"outputs":[],"source":["def selectioncolonne(pandas_df):\n","    '''Cette fonction selectionne les colonnes que l'on veut garder '''\n","    pandas_df_new = pandas_df[[\"N° SIREN\",\"VP.224\",\"VP.225\", \"VP.226\", \"VP.227\",\"VP.228\",\"VP.229\",\"VP.231\", \"VP.232\",\"VP.234\"]]\n","\n","    return pandas_df_new"]},{"cell_type":"markdown","id":"5d0265a1-8ed7-4a25-a817-fba27b14415f","metadata":{},"source":["# Création d'une fonction pour convertir la table pandas en table spark"]},{"cell_type":"code","execution_count":42,"id":"6047f835-12b1-4e09-b02d-99a70c3f777c","metadata":{"tags":[]},"outputs":[],"source":["def convertirPandasEnSpark(pandas_df):\n","    '''Cette fonction converti la table pandas en table spark'''\n","    #Conversion du DataFrame Pandas en DataFrame Spark\n","    table = spark.createDataFrame(pandas_df)\n","    \n","    return table"]},{"cell_type":"markdown","id":"2964776f-095f-4f56-83eb-554373d1b4c7","metadata":{},"source":["# Je renomme les colonnes de mon tableau"]},{"cell_type":"code","execution_count":43,"id":"afa7ffbd-e50f-4750-93aa-22a537524d5a","metadata":{},"outputs":[],"source":["def renommageColonnes(table_spark):\n","    ''' Cette fonction renomme les colonnes de la table '''\n","\n","    table_renamed = table_spark.withColumnRenamed(\"VP.224\", \"Indice linéaire de consommation\")\\\n","        .withColumnRenamed(\"VP.225\", \"Rendement sur les 3 années précédentes\")\\\n","        .withColumnRenamed(\"VP.226\", \"Rendement seuil par défaut\")\\\n","        .withColumnRenamed(\"VP.227\", \"Rendement seuil en ZRE\")\\\n","        .withColumnRenamed(\"VP.228\", \"Densité linéaire d'abonnés\")\\\n","        .withColumnRenamed(\"VP.229\", \"Ratio habitants par abonnés\")\\\n","        .withColumnRenamed(\"VP.231\", \"Consommation moyenne par abonné\")\\\n","        .withColumnRenamed(\"VP.232\", \"Volumes consommés comptabilisés\")\\\n","        .withColumnRenamed(\"VP.234\", \"Volume produit + Volume importé\")\n","    return table_renamed"]},{"cell_type":"code","execution_count":44,"id":"aca891a9-b7b2-47eb-9aac-912944d93606","metadata":{},"outputs":[],"source":["def typage(La_table_spark_que_je_veux_modifier):\n","    \"cette fonction change le typage de la colonne numero Siren\"\n","    La_nouvelle_table_spark = La_table_spark_que_je_veux_modifier.withColumn(\"N° SIREN\", col(\"N° SIREN\").cast(StringType()))\n","    return(La_nouvelle_table_spark)"]},{"cell_type":"markdown","id":"0c1c8782-7387-4805-99b2-fee92f1c5b76","metadata":{},"source":["# Ajout de la colonne année"]},{"cell_type":"code","execution_count":45,"id":"f80a4f6b-9103-497b-b7af-4382d1e532d0","metadata":{},"outputs":[],"source":["def ajoutcol_annee(table_spark,annee):\n","    # Add new constanst column\n","    dataframe_colannee = table_spark.withColumn(\"Annee\", lit(annee))\n","    print(type(dataframe_colannee))\n","    printSchema(dataframe_colannee)\n","    return dataframe_colannee"]},{"cell_type":"markdown","id":"e2435050-78cd-4009-9e7c-1bf9f35df7a7","metadata":{"tags":[]},"source":["# Je sauvegarde mon tableau dans un fichier csv"]},{"cell_type":"code","execution_count":46,"id":"7f4ebd60-ed9a-47a1-a7b1-a55719a7e0da","metadata":{"tags":[]},"outputs":[],"source":["def transformationCSV(table_spark):\n","    ''' Cette fonction enregistre une table spark au format CSV '''\n","    table_spark.write.csv(\"./Local Disk/Data/consommation_eau.csv\", header=True, mode=\"append\")"]},{"cell_type":"code","execution_count":47,"id":"b5689842-1245-41cf-8132-ad120f9cff8f","metadata":{},"outputs":[],"source":["def transformationCSV_dans_bucket(table_spark,annee):\n","    ''' Cette fonction enregistre une table spark en csv dans un bucket GCP'''\n","\n","    nom_fichier = f\"consommation_eau.csv\"\n","    chemin_gcs= f\"gs://{NOM_BUCKET}/{NOM_DOSSIER_DANS_BUCKET}/{nom_fichier}\"\n","\n","    client = storage.Client()\n","    table_spark.write.format(\"csv\").save(chemin_gcs, mode='append')\n","\n","    print(f\"Le fichier {nom_fichier} a été enregistré dans {chemin_gcs}\")"]},{"cell_type":"markdown","id":"f75df28e-92ba-495d-bdaf-319b7af9ab2c","metadata":{},"source":["# Fonction principale pour lancer toutes mes fonctions"]},{"cell_type":"code","execution_count":48,"id":"07678014-bf2c-4974-aeb7-44e5b8c60a48","metadata":{"tags":[]},"outputs":[],"source":["def main(annee):\n","    \n","    TelechargementData()\n","    \n","    pandas_dataframe = lecturePandas(annee)\n","\n","    pandas_dataframe_propre = selectioncolonne(pandas_dataframe)\n","\n","    spark_dataframe = convertirPandasEnSpark(pandas_dataframe_propre)\n","\n","    df_bons_noms = renommageColonnes(spark_dataframe)\n","    \n","    df_bons_types = typage(df_bons_noms)\n","    \n","    dataframe_colannee = ajoutcol_annee(df_bons_types, annee)\n","\n","    transformationCSV_dans_bucket(dataframe_colannee, annee)\n","    "]},{"cell_type":"markdown","id":"ba73d560-f5e9-4529-9b8d-1d7ba3fd0e6b","metadata":{},"source":["# Selection des années à traiter."]},{"cell_type":"code","execution_count":49,"id":"262db5ff-2d8a-4606-9e3a-a7feb30343cd","metadata":{"tags":[]},"outputs":[],"source":["annee_min = 2008\n","anne_max = 2021"]},{"cell_type":"code","execution_count":50,"id":"8bc6b71e-10ef-4a47-96e6-993a0eb21674","metadata":{},"outputs":[],"source":["# Nom du bucket de stockage\n","NOM_BUCKET = \"code_de_source_lake\"\n","# Nom du dossier dans le bucket GCS\n","NOM_DOSSIER_DANS_BUCKET = 'Output_data_consommationeau'\n"]},{"cell_type":"markdown","id":"504f2283-bb3c-46ed-9a2f-ef0082269639","metadata":{},"source":["# Execution du programme"]},{"cell_type":"code","execution_count":51,"id":"ab7052ee-5f4f-4b00-b9c5-ebd7b5f3e9e7","metadata":{"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["----------------------------------------\n","Exception occurred during processing of request from ('127.0.0.1', 44024)\n","Traceback (most recent call last):\n","  File \"/opt/conda/miniconda3/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n","    self.process_request(request, client_address)\n","  File \"/opt/conda/miniconda3/lib/python3.10/socketserver.py\", line 347, in process_request\n","    self.finish_request(request, client_address)\n","  File \"/opt/conda/miniconda3/lib/python3.10/socketserver.py\", line 360, in finish_request\n","    self.RequestHandlerClass(request, client_address, self)\n","  File \"/opt/conda/miniconda3/lib/python3.10/socketserver.py\", line 747, in __init__\n","    self.handle()\n","  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 281, in handle\n","    poll(accum_updates)\n","  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 253, in poll\n","    if func():\n","  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n","    num_updates = read_int(self.rfile)\n","  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 595, in read_int\n","    raise EOFError\n","EOFError\n","----------------------------------------\n","ERROR:root:Exception while sending command.\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n","    raise Py4JNetworkError(\"Answer from Java side is empty\")\n","py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n","    response = connection.send_command(command)\n","  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n","    raise Py4JNetworkError(\n","py4j.protocol.Py4JNetworkError: Error while sending or receiving\n","ERROR:root:Exception while sending command.\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n","    raise Py4JNetworkError(\"Answer from Java side is empty\")\n","py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n","    response = connection.send_command(command)\n","  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n","    raise Py4JNetworkError(\n","py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"]},{"ename":"Py4JError","evalue":"SparkSession does not exist in the JVM","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_3060/1485555607.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mannee\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannee_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manne_max\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannee\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_3060/3149003936.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(annee)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpandas_dataframe_propre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselectioncolonne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_dataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mspark_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvertirPandasEnSpark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_dataframe_propre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdf_bons_noms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenommageColonnes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_dataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_3060/3600409616.py\u001b[0m in \u001b[0;36mconvertirPandasEnSpark\u001b[0;34m(pandas_df)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'''Cette fonction converti la table pandas en table spark'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#Conversion du DataFrame Pandas en DataFrame Spark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activeSession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetActiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data is already a DataFrame\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1720\u001b[0m             message = compute_exception_message(\n\u001b[1;32m   1721\u001b[0m                 \"{0} does not exist in the JVM\".format(name), error_message)\n\u001b[0;32m-> 1722\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPy4JError\u001b[0m: SparkSession does not exist in the JVM"]}],"source":["for annee in range(annee_min,anne_max+1):\n","    main(annee)"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}

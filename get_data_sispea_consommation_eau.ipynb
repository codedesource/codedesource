{"cells":[{"cell_type":"markdown","id":"b3abf2ca-b9e4-4ff4-97ac-09587d51f445","metadata":{},"source":["## Auteur : Mauchaussee Pauline \n","# Date de création : 2023/08/17  \n","# Présentation :** Ce notebook permet de telecharger des données climatique depuis une liste de stations stockée dans un table BigQuerry et d'envoyer les données en format CSV dans un bucket GCP.\n","\n","## Prérequis :\n","# - Sauvegarder le fichier excel sous forme de CSV\n","# - Un bucket gcp pour le stockage de données. (BUCKET_NAME)\n","# - Une table BigQuerry contenant la liste des station (TABLE_ID)\n","# - Un token API infoclimat\n","\n","\n","## Params:\n","# BUCKET_NAME : nom du bucket GCP, DOIT être dans le même projet.\n","# TABLE_IDE : ID de la table BigQuerry, DOIT être dans le même projet.\n","# KEY_PATH : Chemin vers le fichier JSON permettant de ce connecter au service account"]},{"cell_type":"markdown","id":"8f14199f-6523-41a0-ada3-422bbf446f9c","metadata":{},"source":["## Import des librairies"]},{"cell_type":"code","execution_count":51,"id":"c212dc7e-806a-4905-89bf-4cacb2b5d2a2","metadata":{},"outputs":[],"source":["from google.cloud import storage"]},{"cell_type":"code","execution_count":52,"id":"87824d20-ea6b-4e0f-8685-41bc0c912912","metadata":{},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":53,"id":"a11f7c36-c66b-4d6c-8677-60a0996536c4","metadata":{"tags":[]},"outputs":[],"source":["#Importer les bibliothèques nécessaires (pandas et pyspark) dans le script pyspark\n","import pandas as pd"]},{"cell_type":"code","execution_count":54,"id":"ad84177c-c73b-43c1-aab6-ad2a9619cd55","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: openpyxl in /opt/conda/miniconda3/lib/python3.10/site-packages (3.1.2)\n","Requirement already satisfied: et-xmlfile in /opt/conda/miniconda3/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#!pip install openpyxl"]},{"cell_type":"code","execution_count":55,"id":"d5359971-79c0-4367-a509-a0a9643f0a28","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: xlrd in /opt/conda/miniconda3/lib/python3.10/site-packages (2.0.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["#!pip install xlrd"]},{"cell_type":"code","execution_count":56,"id":"73e8e13c-cae2-4c95-9ab1-4677efe47db8","metadata":{"tags":[]},"outputs":[],"source":["import openpyxl"]},{"cell_type":"code","execution_count":57,"id":"a4461f77-6b60-4f43-a8a6-84f4fc9d7b6d","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import lit"]},{"cell_type":"code","execution_count":58,"id":"3df454ae-dc8d-4b85-a7a9-667034f2c835","metadata":{"tags":[]},"outputs":[],"source":["import pyspark.sql\n","from pyspark.sql import *"]},{"cell_type":"code","execution_count":59,"id":"eff94251-05b8-4964-ab56-5eb95f98114d","metadata":{"tags":[]},"outputs":[],"source":["import requests"]},{"cell_type":"code","execution_count":60,"id":"4601cc9f-7fb3-4e12-b650-2cb5dea85139","metadata":{"tags":[]},"outputs":[],"source":["from pyspark import SparkFiles"]},{"cell_type":"code","execution_count":61,"id":"94c4127a-6976-427d-a920-fd8eaae21e22","metadata":{"tags":[]},"outputs":[],"source":["import zipfile\n","import io\n","import os"]},{"cell_type":"markdown","id":"ec35367d-bc66-45d3-b96a-bfc4875e17ab","metadata":{},"source":["# Création d'une fonction pour récupérer de la donnée à partir d'une URL "]},{"cell_type":"code","execution_count":63,"id":"cdd065bf-d67c-49dd-a3ea-abdfc240e6b3","metadata":{"tags":[]},"outputs":[],"source":["def TelechargementData():\n","\n","    url = \"https://services.eaufrance.fr/documents/openData/SISPEA_FR_\"+str(annee)+\"_AEP.zip\"\n","\n","    \n","\n","    reponse = requests.get(url)\n","\n","    content = reponse.content\n","\n","    # Création d'un objet ZipFile à partir du contenu ZIP téléchargé\n","    zip_file = zipfile.ZipFile(io.BytesIO(content))\n","\n","    # Création du répertoire de destination s'il n'existe pas\n","    os.makedirs(\"Data\", exist_ok=True)\n","\n","    # Extraction des fichiers du fichier ZIP et enregistrement dans le répertoire de destination\n","    zip_file.extractall(\"Data\")\n","    \n","    # Fermeture du fichier ZIP\n","    zip_file.close()"]},{"cell_type":"markdown","id":"b6601be6-91fb-4faf-8907-399e6b5dc146","metadata":{},"source":["# Création d'une fonction pour lire mon fichier excel et conversion en un DataFrame Pandas"]},{"cell_type":"code","execution_count":64,"id":"b9da2b25-4b47-4126-b75d-ff002a0b839b","metadata":{"tags":[]},"outputs":[],"source":["def lecturePandas(annee):\n","    ''' Cette fonction permet de lire un fichier excel et de le convertir en dataFrame Pandas'''\n","\n","    #Pour lire le fichier excel on utilise Pandas \n","    #On donne le chemin du fichier excel\n","    excel_file_path = \"./Data/SISPEA_FR_\"+str(annee)+\"_AEP.xls\"\n","    #On donne le nom de l'onglet\n","    sheet_name = \"Entités de gestion\"\n","    #Lecture du fichier excel avec pandas \n","    pandas_df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n","\n","    return pandas_df"]},{"cell_type":"markdown","id":"cea0d18e-2389-445c-9871-4da473fecec4","metadata":{},"source":["# Je sélectionne les colonnes que l'on souhaite garder"]},{"cell_type":"code","execution_count":75,"id":"f9b23b6b-eca4-461b-be49-1389a088e77d","metadata":{},"outputs":[],"source":["def selectioncolonne(pandas_df):\n","    '''Cette fonction selectionne les colonnes que l'on veut garder '''\n","    pandas_df_new = pandas_df[[\"N° SIREN\",\"VP.224\",\"VP.225\", \"VP.226\", \"VP.227\",\"VP.228\",\"VP.229\",\"VP.231\", \"VP.232\",\"VP.234\"]]\n","\n","    return pandas_df_new"]},{"cell_type":"markdown","id":"5d0265a1-8ed7-4a25-a817-fba27b14415f","metadata":{},"source":["# Création d'une fonction pour convertir la table pandas en table spark"]},{"cell_type":"code","execution_count":76,"id":"6047f835-12b1-4e09-b02d-99a70c3f777c","metadata":{"tags":[]},"outputs":[],"source":["def convertirPandasEnSpark(pandas_df):\n","    '''Cette fonction converti la table pandas en table spark'''\n","    #Conversion du DataFrame Pandas en DataFrame Spark\n","    table = spark.createDataFrame(pandas_df)\n","    \n","    return table"]},{"cell_type":"markdown","id":"2964776f-095f-4f56-83eb-554373d1b4c7","metadata":{},"source":["# Je renomme les colonnes de mon tableau"]},{"cell_type":"code","execution_count":67,"id":"afa7ffbd-e50f-4750-93aa-22a537524d5a","metadata":{},"outputs":[],"source":["def renommageColonnes(table_spark):\n","    ''' Cette fonction renomme les colonnes de la table '''\n","\n","    table_renamed = table_spark.withColumnRenamed(\"VP.224\", \"Indice linéaire de consommation\")\\\n","        .withColumnRenamed(\"VP.225\", \"Rendement sur les 3 années précédentes\")\\\n","        .withColumnRenamed(\"VP.226\", \"Rendement seuil par défaut\")\\\n","        .withColumnRenamed(\"VP.227\", \"Rendement seuil en ZRE\")\\\n","        .withColumnRenamed(\"VP.228\", \"Densité linéaire d'abonnés\")\\\n","        .withColumnRenamed(\"VP.229\", \"Ratio habitants par abonnés\")\\\n","        .withColumnRenamed(\"VP.231\", \"Consommation moyenne par abonné\")\\\n","        .withColumnRenamed(\"VP.232\", \"Volumes consommés comptabilisés\")\\\n","        .withColumnRenamed(\"VP.234\", \"Volume produit + Volume importé\")\n","    return table_renamed"]},{"cell_type":"markdown","id":"0c1c8782-7387-4805-99b2-fee92f1c5b76","metadata":{},"source":["# Ajout de la colonne année"]},{"cell_type":"code","execution_count":68,"id":"f80a4f6b-9103-497b-b7af-4382d1e532d0","metadata":{},"outputs":[],"source":["def ajoutcol_annee(table_spark,annee):\n","    # Add new constanst column\n","    dataframe_colannee = table_spark.withColumn(\"Annee\", lit(annee))\n","    return dataframe_colannee"]},{"cell_type":"markdown","id":"e2435050-78cd-4009-9e7c-1bf9f35df7a7","metadata":{"tags":[]},"source":["# Je sauvegarde mon tableau dans un fichier csv"]},{"cell_type":"code","execution_count":69,"id":"7f4ebd60-ed9a-47a1-a7b1-a55719a7e0da","metadata":{"tags":[]},"outputs":[],"source":["def transformationCSV(table_spark):\n","    ''' Cette fonction enregistre une table spark au format CSV '''\n","    table_spark.write.csv(\"./Local Disk/Data/consommation_eau.csv\", header=True, mode=\"append\")"]},{"cell_type":"markdown","id":"f75df28e-92ba-495d-bdaf-319b7af9ab2c","metadata":{},"source":["# Fonction principale pour lancer toutes mes fonctions"]},{"cell_type":"code","execution_count":78,"id":"07678014-bf2c-4974-aeb7-44e5b8c60a48","metadata":{"tags":[]},"outputs":[],"source":["def main(annee):\n","    \n","    TelechargementData()\n","    \n","    pandas_dataframe = lecturePandas(annee)\n","\n","    pandas_dataframe_propre = selectioncolonne(pandas_dataframe)\n","\n","    spark_dataframe = convertirPandasEnSpark(pandas_dataframe_propre)\n","\n","    df_bons_noms = renommageColonnes(spark_dataframe)\n","    \n","    dataframe_colannee = ajoutcol_annee(df_bons_noms, annee)\n","\n","    transformationCSV(dataframe_colannee)\n","    "]},{"cell_type":"markdown","id":"ba73d560-f5e9-4529-9b8d-1d7ba3fd0e6b","metadata":{},"source":["# Selection des années à traiter."]},{"cell_type":"code","execution_count":79,"id":"262db5ff-2d8a-4606-9e3a-a7feb30343cd","metadata":{"tags":[]},"outputs":[],"source":["annee_min = 2008\n","anne_max = 2021"]},{"cell_type":"markdown","id":"504f2283-bb3c-46ed-9a2f-ef0082269639","metadata":{},"source":["# Execution du programme"]},{"cell_type":"code","execution_count":80,"id":"ab7052ee-5f4f-4b00-b9c5-ebd7b5f3e9e7","metadata":{"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["for annee in range(annee_min,anne_max+1):\n","    main(annee)"]},{"cell_type":"markdown","id":"88651a98-b702-41d4-afdb-385029bf7a9f","metadata":{},"source":["# Envoie des fichiers csv dans le bucket"]},{"cell_type":"code","execution_count":null,"id":"e632cb24-c81c-4194-a0ff-8bc6cd689c8a","metadata":{},"outputs":[],"source":["## lien URI du bucket crée \n","DATALAKE_PATH = \"gs://code_de_source_lake\"\n"]},{"cell_type":"code","execution_count":null,"id":"b26419d0-5c98-4886-a8c3-c5da5e910d43","metadata":{},"outputs":[],"source":["## variables pour envoyer mon fichier csv sur mon bucket\n","\n","BUCKET_NAME = \"code_de_source_lake\"\n","local_folder_path = \"./Data/Output_data\"\n","gcs_file_path = \"consommation_eau.csv\"\n","\n"]},{"cell_type":"code","execution_count":null,"id":"20a1a67f-9dcb-4f44-b59f-85295c767722","metadata":{},"outputs":[],"source":["# Nom du dossier dans le bucket GCS\n","gcs_folder_name = 'Output_data_consommationeau'"]},{"cell_type":"code","execution_count":null,"id":"7d3885b8-7388-44bf-a73f-db654911d847","metadata":{},"outputs":[],"source":["# Initialiser le client de stockage GCP\n","client = storage.Client()"]},{"cell_type":"code","execution_count":null,"id":"89051f62-5bf7-48fd-a56e-a695268f2a38","metadata":{},"outputs":[],"source":["# Récupérer le bucket\n","bucket = client.bucket(BUCKET_NAME)\n"]},{"cell_type":"code","execution_count":null,"id":"0b0d5c31-b670-4498-8b1c-fccf40055738","metadata":{},"outputs":[],"source":["# Envoyer le fichier dans le bucket\n","blob = bucket.blob(gcs_file_path)"]},{"cell_type":"code","execution_count":null,"id":"de2123f0-68f8-4a33-82b1-795be452af0d","metadata":{},"outputs":[],"source":["# Parcourir les fichiers du dossier local\n","for root, dirs, files in os.walk(local_folder_path):\n","    for file in files:\n","        local_file_path = os.path.join(root, file)\n","        gcs_file_path = os.path.join(gcs_folder_name, os.path.relpath(local_file_path, local_folder_path))\n","        \n","        # Créer un objet Blob dans le bucket\n","        blob = bucket.blob(gcs_file_path)\n","        \n","        # Charger le contenu du fichier local dans le Blob\n","        blob.upload_from_filename(local_file_path)\n","\n","        print(f'Fichier {local_file_path} envoyé vers {gcs_file_path}.')"]},{"cell_type":"code","execution_count":null,"id":"bdb1a657-a0a2-4fcc-a27d-193ea500b17e","metadata":{},"outputs":[],"source":["# J'ouvre mon fichier pour le lire en binaire\n","#with open (local_file_path, \"rb\") as fichier:\n","#J'upload le contenu de mon fichier dans mon blob\n","    #blob.upload_from_file(fichier)\n","\n","#print(f\"Le fichier {local_file_path} a été upload dans le bucket {BUCKET_NAME}.\")"]},{"cell_type":"code","execution_count":null,"id":"a57d3e4c-ba7e-40d2-abe9-4ad305a11c4b","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}

{"cells": [{"cell_type": "markdown", "id": "a7e36dd7-c363-42b6-b5bb-1953f2454645", "metadata": {}, "source": "# Transformation des donn\u00e9es infoclimat"}, {"cell_type": "markdown", "id": "988c4de8-641d-4734-b226-0766c922fd25", "metadata": {}, "source": "**Auteur :**  Steve Caron  \n**Date de cr\u00e9ation :** 2023/08/23  \n**Pr\u00e9sentation :** Ce notebook transforme des donn\u00e9es stock\u00e9es dans un bucket GCP et les envoie vers BigQuery\n\n\n**Pr\u00e9requis :**  \n* Un bucket gcp contenant des donn\u00e9es au format CSV\n* Une table BigQuery contenant les information sur les stations m\u00e9t\u00e9o (code, nom, coordonn\u00e9es)  \n* Une table BigQuery pouvant accueillir les donn\u00e9es\n\n\n**Pr\u00e9requis :** \n- Un bucket gcp pour le stockage de donn\u00e9es. (BUCKET_NAME)\n- Une table BigQuerry contenant la liste des station (TABLE_ID_INPUT)\n- Une table BigQuerry sur laquelle envoyer les donn\u00e9es finales (ID_TABLE_OUTPUT)\n\n**Inputs :** \n\n**Params:**\n* BUCKET_NAME : nom du bucket GCP, doit \u00eatre dans le m\u00eame projet.\n* TABLE_ID_INPUT : ID de la table BigQuery, doit \u00eatre dans le m\u00eame projet.\n* Table_ID_OUTPUT : ID de la table BigQuery, doit \u00eatre dans le m\u00eame projet."}, {"cell_type": "markdown", "id": "4c6bc7ba-c194-46b9-a0e6-715cc6c2d33b", "metadata": {}, "source": "# Import des librairies"}, {"cell_type": "code", "execution_count": 1, "id": "5533debb-7c9e-47fc-b4ac-e6870851426f", "metadata": {}, "outputs": [], "source": "from google.cloud import bigquery\nfrom google.cloud import storage\nimport os\nfrom pyspark.sql.functions import col,sum,avg,max,min,round,lit\nfrom pyspark.sql.types import StringType,BooleanType,DateType,FloatType,IntegerType,StructType,StructField,DecimalType"}, {"cell_type": "markdown", "id": "74a88162-01c2-49f0-93d7-fd00bb7d0c78", "metadata": {}, "source": "# Definition des Param\u00e8tres"}, {"cell_type": "code", "execution_count": 2, "id": "4b8d7b8f-e9e3-4759-bdd5-f1eae19dbbf6", "metadata": {}, "outputs": [], "source": "NOM_BUCKET = 'code_de_source_lake'\nNOM_REPERTOIRE = \"infoclimat\"\nID_TABLE_INPUT = \"code-de-source.donnees_code_de_source.stations_meteo\"\nID_TABLE_OUTPUT = \"code-de-source.donnees_code_de_source.donnees_meteo\"\nNOM_BUCKET_TEMP = \"traitement_meteo_temp\""}, {"cell_type": "markdown", "id": "b8a17337-23fc-405e-a6ea-8c82fe82db3a", "metadata": {}, "source": "# Recup\u00e9ration des fichiers dans le Bucket"}, {"cell_type": "code", "execution_count": 3, "id": "0fbd112d-e1e8-4e23-a38a-32ccc82f6ba3", "metadata": {}, "outputs": [], "source": "# Initialisation de la liste de fichier\nnoms_fichier = []\n\n# Definition du delimiter\ndelimiter = None\n\n# Initialisation du client\nstorage_client = storage.Client()\n\n# Note: Client.list_blobs requires at least package version 1.17.0.\nblobs = storage_client.list_blobs(NOM_BUCKET, prefix=NOM_REPERTOIRE, delimiter=delimiter)\n\n# Parcourir les objets (fichiers) dans le bucket\nfor blob in blobs:\n    noms_fichier.append(\"gs://{}/{}\".format(NOM_BUCKET,blob.name))"}, {"cell_type": "code", "execution_count": 4, "id": "c8205aec-9e00-4de5-9b28-6039110c7c58", "metadata": {}, "outputs": [], "source": "# print(noms_fichier)"}, {"cell_type": "markdown", "id": "806a1e1e-b271-47dc-8483-32ae80160ba3", "metadata": {}, "source": "# Lecture des fichier avec Spark"}, {"cell_type": "code", "execution_count": 5, "id": "b1f3bc0f-6ac8-4c2a-b84d-36ecb1d83b5c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/08/23 13:56:26 WARN GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=497; previousMaxLatencyMs=422; operationCount=50; context=gs://code_de_source_lake/infoclimat/m\u00e9t\u00e9o_000EN_2022.csv\n23/08/23 13:56:26 WARN GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=583; previousMaxLatencyMs=497; operationCount=50; context=gs://code_de_source_lake/infoclimat/m\u00e9t\u00e9o_000EN_2023.csv\n23/08/23 13:56:26 WARN GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=618; previousMaxLatencyMs=583; operationCount=50; context=gs://code_de_source_lake/infoclimat/m\u00e9t\u00e9o_000EN_2017.csv\n23/08/23 13:56:26 WARN GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=730; previousMaxLatencyMs=618; operationCount=50; context=gs://code_de_source_lake/infoclimat/m\u00e9t\u00e9o_000BS_2020.csv\n                                                                                \r"}], "source": "donnees_brut = spark.read \\\n    .option(\"header\", \"true\") \\\n    .csv(noms_fichier)"}, {"cell_type": "code", "execution_count": 6, "id": "cf9ae31e-113b-4fba-b424-29513c515097", "metadata": {}, "outputs": [], "source": "# donnees_brut.show(1)\n# print(type(donnees_brut))"}, {"cell_type": "markdown", "id": "e678d0cc-63bd-4468-ab66-b3310075c4a7", "metadata": {}, "source": "# Renommer les colonnes"}, {"cell_type": "code", "execution_count": 7, "id": "89152d30-5188-46ec-a762-39c6ae054852", "metadata": {}, "outputs": [], "source": "donnees_renommees = donnees_brut.withColumnRenamed(\"station_id\",\"code_station\")\\\n           .withColumnRenamed(\"dh_utc\",\"date\")\\\n           .withColumnRenamed(\"vent_moyen\",\"vitesse_du_vent\")\\\n           .withColumnRenamed(\"vent_direction\",\"direction_du_vent\")\\\n           .withColumnRenamed(\"pluie_3h\",\"precipitation_3h\")\\\n           .withColumnRenamed(\"pluie_1h\",\"precipitation_1h\")"}, {"cell_type": "code", "execution_count": 8, "id": "2e45d5ff-8c01-40ed-a14f-4c2d9b0e8162", "metadata": {}, "outputs": [], "source": "# donnees_renommees.printSchema()\n# print(type(donnees_renommees))"}, {"cell_type": "markdown", "id": "ed76aa56-c0db-4f89-8a24-f8d13509603a", "metadata": {}, "source": "# Typage des colonnes"}, {"cell_type": "code", "execution_count": 9, "id": "a72b30cd-7e74-4c8a-bdf5-4c42155b5d5d", "metadata": {}, "outputs": [], "source": "donnees_typees = donnees_renommees.withColumn(\"date\",col(\"date\").cast(DateType()))\\\n    .withColumn(\"temperature\",col(\"temperature\").cast(DecimalType(10,1)))\\\n    .withColumn(\"pression\",col(\"pression\").cast(IntegerType()))\\\n    .withColumn(\"humidite\",col(\"humidite\").cast(IntegerType()))\\\n    .withColumn(\"point_de_rosee\",col(\"point_de_rosee\").cast(DecimalType(10,1)))\\\n    .withColumn(\"vitesse_du_vent\",col(\"vitesse_du_vent\").cast(DecimalType(10,2)))\\\n    .withColumn(\"vent_rafales\",col(\"vent_rafales\").cast(DecimalType(10,2)))\\\n    .withColumn(\"direction_du_vent\",col(\"direction_du_vent\").cast(IntegerType()))\\\n    .withColumn(\"precipitation_3h\",col(\"precipitation_3h\").cast(DecimalType(10,1)))\\\n    .withColumn(\"precipitation_1h\",col(\"precipitation_1h\").cast(DecimalType(10,1)))"}, {"cell_type": "code", "execution_count": 10, "id": "0e60e5b9-b501-480c-8859-27996a290838", "metadata": {}, "outputs": [], "source": "#  donnees_typees.printSchema()\n# donnees_typees.show(1)\n# print(type(donnees_typees))"}, {"cell_type": "markdown", "id": "00d4db7f-164e-454d-9190-eef9fd5b91bb", "metadata": {}, "source": "# Ajout d'une colone pour la temperature max, la temperature min, la pression max, la pression min, la vitesse du vent max"}, {"cell_type": "code", "execution_count": 11, "id": "c9f716fb-258f-42d4-aa3a-aa54dd62f9bd", "metadata": {}, "outputs": [], "source": "donnees_ajout_col = donnees_typees.withColumn(\"temperature_max\",col(\"temperature\"))\\\n.withColumn(\"temperature_min\",col(\"temperature\"))\\\n.withColumn(\"pression_max\",col(\"pression\"))\\\n.withColumn(\"pression_min\",col(\"pression\"))\\\n.withColumn(\"vitesse_du_vent_max\",col(\"vitesse_du_vent\"))"}, {"cell_type": "code", "execution_count": 12, "id": "7a6d928e-56fa-40ed-8daa-c698d62f4d39", "metadata": {}, "outputs": [], "source": "# donnees_ajout_col.printSchema()\n# print(type(donnees_ajout_col))"}, {"cell_type": "markdown", "id": "b6413b9b-33c6-4ffa-8c36-0a11755eec27", "metadata": {}, "source": "# Aggregation par date"}, {"cell_type": "code", "execution_count": 13, "id": "3e160773-5111-412d-913b-b50f546a2bea", "metadata": {}, "outputs": [], "source": "donnees_aggregees = donnees_ajout_col.groupBy(\"code_station\",\"date\") \\\n    .agg(avg(\"temperature\").cast(DecimalType(10,1)).alias(\"temperature\"), \\\n         min(\"temperature_min\").alias(\"temperature_min\"), \\\n         max(\"temperature_max\").alias(\"temperature_max\"), \\\n         avg(\"pression\").cast(IntegerType()).alias(\"pression\"),\\\n         min(\"pression_min\").cast(IntegerType()).alias(\"pression_min\"), \\\n         max(\"pression_max\").cast(IntegerType()).alias(\"pression_max\"), \\\n         avg(\"humidite\").cast(IntegerType()).alias(\"humidite\"),\\\n         avg(\"point_de_rosee\").cast(DecimalType(10,1)).alias(\"point_de_rosee\"),\\\n         avg(\"vitesse_du_vent\").cast(DecimalType(10,2)).alias(\"vitesse_du_vent\"),\\\n         max(\"vitesse_du_vent_max\").alias(\"vitesse_du_vent_max\"), \\\n         avg(\"direction_du_vent\").cast(IntegerType()).alias(\"direction_du_vent\"),\\\n         sum(\"precipitation_3h\").alias(\"precipitation\")) \\\n        .orderBy(\"date\")"}, {"cell_type": "code", "execution_count": 14, "id": "684af928-7d95-47d8-acb9-f8fc7147f4bf", "metadata": {}, "outputs": [], "source": "# donnees_aggregees.show(5)\n# print(type(donnees_aggregees))"}, {"cell_type": "markdown", "id": "218e2b76-3aae-4b57-85c1-cbf948fa9978", "metadata": {}, "source": "# Collecte des informations sur les stations"}, {"cell_type": "code", "execution_count": 15, "id": "550a1227-c631-4171-92e2-4ed636d4d475", "metadata": {}, "outputs": [], "source": "stations = spark.read.format('bigquery') \\\n  .option('table', ID_TABLE_INPUT) \\\n  .load()\n\nstations = stations.select(\"code_station\", \"nom_station\", \"coordonnees_x\", \"coordonnees_y\")"}, {"cell_type": "code", "execution_count": 16, "id": "50f9dbe0-5374-40e1-906b-495abf9e5540", "metadata": {}, "outputs": [], "source": "# stations.printSchema()\n# stations.show(5)\n# print(type(stations))"}, {"cell_type": "markdown", "id": "f9cd495d-c19e-4a5e-a0ef-ed7c86ddcd2b", "metadata": {}, "source": "# Jointure des tables"}, {"cell_type": "code", "execution_count": 17, "id": "0b4b8660-34ac-4f4c-9cdb-f454e7ba651d", "metadata": {}, "outputs": [], "source": "data_jointes = donnees_aggregees.join(stations,\n                                      donnees_aggregees[\"code_station\"] == stations[\"code_station\"]\n                                     ).drop(stations[0])\\\n                                     .withColumnRenamed(\"coordonnees_x\",\"longitude_x\")\\\n                                     .withColumnRenamed(\"coordonnees_y\",\"latitude_y\")\\\n                                     .withColumnRenamed(\"nom_station\",\"localite\")\\\n                                     .withColumn(\"localite\",col(\"localite\").cast(StringType()))\\\n                                     .withColumn(\"longitude_x\",col(\"longitude_x\").cast(FloatType()))\\\n                                     .withColumn(\"latitude_y\",col(\"latitude_y\").cast(FloatType()))\n\ndata_jointes = data_jointes.select(\"date\",\"localite\",\"code_station\",\\\n                                   \"longitude_x\",\"latitude_y\",\\\n                                   \"temperature\",\"temperature_min\",\"temperature_max\",\\\n                                   \"pression\",\"pression_min\",\"pression_max\",\\\n                                   \"humidite\",\"point_de_rosee\",\\\n                                   \"vitesse_du_vent\",\"vitesse_du_vent_max\",\"direction_du_vent\",\\\n                                   \"precipitation\")\\\n                                    .withColumn(\"temperature\",col(\"temperature\").cast(DecimalType(10,1)))\\\n                                    .withColumn(\"temperature\",col(\"temperature\").cast(DecimalType(10,1)))\\\n                                    .withColumn(\"precipitation\",col(\"precipitation\").cast(DecimalType(10,1)))\\\n                                    .withColumn(\"humidite\",col(\"humidite\").cast(DecimalType(10,1)))\\\n                                    .withColumn(\"vitesse_du_vent\",col(\"vitesse_du_vent\").cast(DecimalType(10,1)))\\\n                                    .withColumn(\"vitesse_du_vent_max\",col(\"vitesse_du_vent_max\").cast(DecimalType(10,1)))\n"}, {"cell_type": "code", "execution_count": 18, "id": "84e5f8b1-b011-461e-9ca3-e85ae3908866", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- date: date (nullable = true)\n |-- localite: string (nullable = true)\n |-- code_station: string (nullable = true)\n |-- longitude_x: float (nullable = true)\n |-- latitude_y: float (nullable = true)\n |-- temperature: decimal(10,1) (nullable = true)\n |-- temperature_min: decimal(10,1) (nullable = true)\n |-- temperature_max: decimal(10,1) (nullable = true)\n |-- pression: integer (nullable = true)\n |-- pression_min: integer (nullable = true)\n |-- pression_max: integer (nullable = true)\n |-- humidite: decimal(10,1) (nullable = true)\n |-- point_de_rosee: decimal(10,1) (nullable = true)\n |-- vitesse_du_vent: decimal(10,1) (nullable = true)\n |-- vitesse_du_vent_max: decimal(10,1) (nullable = true)\n |-- direction_du_vent: integer (nullable = true)\n |-- precipitation: decimal(10,1) (nullable = true)\n\n"}], "source": "data_jointes.printSchema()\n# data_finales.show(1)\n# print(type(data_finales))"}, {"cell_type": "markdown", "id": "e6dc22d7-2998-4331-8a56-dd7d85c4e593", "metadata": {}, "source": "# Enregistrement dans une Table"}, {"cell_type": "code", "execution_count": 19, "id": "11165033-a754-403c-9180-23eb0ed40e11", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/08/23 13:57:31 WARN GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=190; previousMaxLatencyMs=0; operationCount=1; context=gs://traitement_meteo_temp/.spark-bigquery-application_1692795747682_0005-d2ab9648-3da5-495c-b065-e81221760d79/_temporary\n23/08/23 13:57:31 WARN GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=111; previousMaxLatencyMs=0; operationCount=1; context=gs://traitement_meteo_temp/.spark-bigquery-application_1692795747682_0005-d2ab9648-3da5-495c-b065-e81221760d79/_SUCCESS\nERROR:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n    raise Py4JNetworkError(\"Answer from Java side is empty\")\npy4j.protocol.Py4JNetworkError: Answer from Java side is empty\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending or receiving\n/usr/lib/spark/python/pyspark/context.py:561: RuntimeWarning: Unable to cleanly shutdown Spark JVM process. It is possible that the process has crashed, been killed or may also be in a zombie state.\n  warnings.warn(\nERROR:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n    raise Py4JNetworkError(\"Answer from Java side is empty\")\npy4j.protocol.Py4JNetworkError: Answer from Java side is empty\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending or receiving\n"}], "source": "data_jointes.write \\\n  .format(\"bigquery\") \\\n  .option(\"table\",ID_TABLE_OUTPUT)\\\n  .option(\"temporaryGcsBucket\",NOM_BUCKET_TEMP) \\\n  .mode(\"append\") \\\n  .save()"}, {"cell_type": "code", "execution_count": null, "id": "c266ca50-9e68-4de0-ae16-9bb00f16284b", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}